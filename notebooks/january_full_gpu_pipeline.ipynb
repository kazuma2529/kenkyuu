{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# January-kenkyuu GPU/CPU フルパイプライン実行ノート\n",
        "\n",
        "# このノートの目的:\n",
        "# - Google Drive をマウント\n",
        "# - リポジトリを clone して January-kenkyuu ブランチをチェックアウト\n",
        "# - 依存パッケージをインストール\n",
        "# - CLI ランナー (scripts/run_full_pipeline_cli.py) を CPU / GPU で実行\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Notebook for January-kenkyuu full pipeline (CPU/GPU) setup\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf /content/drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Google Drive をマウント\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. リポジトリを clone し、January-kenkyuu ブランチをチェックアウト\n",
        "\n",
        "%cd /content\n",
        "\n",
        "# TODO: あなたの GitHub リポジトリURL に書き換えてください\n",
        "REPO_URL = \"https://github.com/your-account/kenkyuu.git\"\n",
        "\n",
        "!git clone \"$REPO_URL\" kenkyuu\n",
        "%cd /content/kenkyuu\n",
        "\n",
        "!git fetch\n",
        "!git checkout January-kenkyuu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. 依存パッケージのインストール\n",
        "\n",
        "%pip install -q -r requirements.txt\n",
        "\n",
        "# GPU 用ライブラリ（PyTorch）をインストール\n",
        "# PyTorch は CUDA ランタイムを内包しているため、Colab 環境で動作しやすい\n",
        "# 既に requirements.txt に含まれていますが、明示的にインストール\n",
        "%pip install -q torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. GPU が使えるか確認（PyTorch）\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(\"✅ PyTorch インストール成功\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    \n",
        "    # CUDA が利用可能か確認\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"✅ CUDA が利用可能です\")\n",
        "        print(f\"CUDA version: {torch.version.cuda}\")\n",
        "        print(f\"GPU デバイス数: {torch.cuda.device_count()}\")\n",
        "        \n",
        "        # 現在のGPU情報\n",
        "        current_device = torch.cuda.current_device()\n",
        "        print(f\"現在のGPU: {torch.cuda.get_device_name(current_device)}\")\n",
        "        \n",
        "        # GPU メモリ情報\n",
        "        gpu_memory = torch.cuda.get_device_properties(current_device).total_memory / 1024**3\n",
        "        print(f\"GPU メモリ: {gpu_memory:.2f} GB\")\n",
        "        \n",
        "        # 簡単なテスト\n",
        "        test_tensor = torch.tensor([1, 2, 3], device='cuda')\n",
        "        print(f\"GPU テストテンソル: {test_tensor}\")\n",
        "        print(\"✅ GPU が使用可能です\")\n",
        "    else:\n",
        "        print(\"⚠️ CUDA が利用できません。CPU モードで実行します\")\n",
        "        print(\"GPU が割り当てられていない可能性があります。\")\n",
        "        print(\"Colab の「ランタイム」→「ランタイムのタイプを変更」でGPUを有効化してください。\")\n",
        "        \n",
        "except ImportError as e:\n",
        "    print(\"❌ PyTorch のインポートに失敗しました\")\n",
        "    print(\"エラー:\", e)\n",
        "    print(\"CPU モードで実行します\")\n",
        "except Exception as e:\n",
        "    print(\"⚠️ GPU 確認中にエラー:\", e)\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. 入力フォルダ / 出力フォルダの設定（Google Drive 上）\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# TODO: あなたの Drive 上の TIF フォルダに書き換えてください\n",
        "INPUT_FOLDER = \"/content/drive/MyDrive/CT_data\"\n",
        "\n",
        "OUTPUT_BASE = \"/content/drive/MyDrive/kenkyuu_results\"\n",
        "Path(OUTPUT_BASE).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"INPUT_FOLDER =\", INPUT_FOLDER)\n",
        "print(\"OUTPUT_BASE =\", OUTPUT_BASE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. （オプション）CPU モードで軽く確認（r=1 だけ）\n",
        "\n",
        "# まずは r=1 だけで動作確認したい場合はこのセルを実行\n",
        "# !python scripts/run_full_pipeline_cli.py \\\n",
        "#   --input \"$INPUT_FOLDER\" \\\n",
        "#   --output \"$OUTPUT_BASE/cli_cpu_test_r1\" \\\n",
        "#   --max-radius 1 \\\n",
        "#   --connectivity 6 \\\n",
        "#   --threshold-method otsu \\\n",
        "#   --tau-ratio 0.05 \\\n",
        "#   --tau-gain-rel 0.003 \\\n",
        "#   --contacts-min 4 \\\n",
        "#   --contacts-max 10 \\\n",
        "#   --backend cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. GPU モードで r=10 までフル実行（爆速版）\n",
        "\n",
        "# これがメインの実行セルです\n",
        "# GPU を使って r=1 から r=10 まで一気に回します\n",
        "\n",
        "!python scripts/run_full_pipeline_cli.py \\\n",
        "  --input \"$INPUT_FOLDER\" \\\n",
        "  --output \"$OUTPUT_BASE/gpu_full_r10\" \\\n",
        "  --max-radius 10 \\\n",
        "  --connectivity 6 \\\n",
        "  --threshold-method otsu \\\n",
        "  --tau-ratio 0.05 \\\n",
        "  --tau-gain-rel 0.003 \\\n",
        "  --contacts-min 4 \\\n",
        "  --contacts-max 10 \\\n",
        "  --backend gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. GPU モードで複数パラメータを一気に試す（PDCA用）\n",
        "\n",
        "# 二値化パラメータを変えながら、GPU で一気に回して比較します\n",
        "# 結果の PNG を Drive で見比べて、最適な設定を見つけます\n",
        "\n",
        "runs = [\n",
        "    dict(name=\"gpu_otsu_no_clahe\",  threshold=\"otsu\",     clahe=False),\n",
        "    dict(name=\"gpu_otsu_clahe\",     threshold=\"otsu\",     clahe=True),\n",
        "    dict(name=\"gpu_triangle_no_clahe\", threshold=\"triangle\", clahe=False),\n",
        "    dict(name=\"gpu_triangle_clahe\", threshold=\"triangle\", clahe=True),\n",
        "]\n",
        "\n",
        "for cfg in runs:\n",
        "    out_dir = f\"{OUTPUT_BASE}/{cfg['name']}\"\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Running: {cfg['name']}\")\n",
        "    print(f\"  threshold={cfg['threshold']}, clahe={cfg['clahe']}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    clahe_flag = \"--enable-clahe\" if cfg[\"clahe\"] else \"\"\n",
        "\n",
        "    !python scripts/run_full_pipeline_cli.py \\\n",
        "      --input \"$INPUT_FOLDER\" \\\n",
        "      --output \"$out_dir\" \\\n",
        "      --max-radius 10 \\\n",
        "      --connectivity 6 \\\n",
        "      --threshold-method {cfg[\"threshold\"]} \\\n",
        "      --tau-ratio 0.05 \\\n",
        "      --tau-gain-rel 0.003 \\\n",
        "      --contacts-min 4 \\\n",
        "      --contacts-max 10 \\\n",
        "      --backend gpu \\\n",
        "      $clahe_flag\n",
        "    \n",
        "    print(f\"✅ {cfg['name']} 完了\\n\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
